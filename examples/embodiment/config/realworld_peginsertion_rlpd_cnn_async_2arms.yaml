defaults:
  - env/realworld_peg_insertion@env.train
  - env/realworld_peg_insertion@env.eval
  - model/cnn_policy@actor.model
  - training_backend/fsdp@actor.fsdp_config
  - override hydra/job_logging: stdout

hydra:
  run:
    dir: .
  output_subdir: null
  searchpath:
    - file://${oc.env:EMBODIED_PATH}/config/

cluster:
  num_nodes: 3
  component_placement:
    actor: 
      node_group: "4090"
      placement: 0
    env:
      node_group: franka
      placement: 0-1
    rollout:
      node_group: "4090"
      placement: 0:0-1
  node_groups:
    - label: "4090"
      node_ranks: 0-0
    - label: franka
      node_ranks: 1-2
      hardware:
        type: Franka
        configs:
          - robot_ip: ROBOT_IP_FOR_RANK1
            node_rank: 1
          - robot_ip: ROBOT_IP_FOR_RANK2
            node_rank: 2

runner:
  task_type: embodied
  logger:
    log_path: "../results"
    project_name: rlinf
    experiment_name: "sac_mlp"
    logger_backends: ["tensorboard"] # wandb, swanlab

  max_epochs: 10
  max_steps: -1

  only_eval: False
  val_check_interval: -1
  save_interval: -1

  resume_dir: null # Optional: path to a saved checkpoint directory, such as 'checkpoints/global_step_10'. If not None, it will be used to resume training.
  ckpt_path: null  # Optional: path to a .pt checkpoint. If not None, it will be loaded after the model is instantiated (for evaluation).

algorithm:
  update_epoch: 32
  group_size: 1
  agg_q: min # or mean
  actor_agg_q: mean
  enable_drq: False

  rollout_epoch: 1
  eval_rollout_epoch: 1

  adv_type: embodied_sac
  loss_type: embodied_sac
  loss_agg_func: "token-mean"
  
  bootstrap_type: always
  gamma: 0.8
  tau: 0.01  # Soft update coefficient for target networks
  
  entropy_tuning:
    alpha_type: softplus # choose from ["softplus", "exp", "fixed_alpha"], set to "softplus" or "exp" to enable automatic entropy tuning
    initial_alpha: 0.01  # Initial temperature value
    target_entropy: -4  # Target entropy (-action_dim)
    optim:
      lr: 3.0e-4  # Learning rate for temperature parameter
      lr_scheduler: torch_constant
      clip_grad: 10.0
  
  # Replay buffer settings
  replay_buffer:
    enable_cache: True
    cache_size: 200 # number of trajectories cached in memory
    sample_window_size: 200 # number of latest trajectories to sample from for replay buffer
    min_buffer_size: 2  # Minimum buffer size before training starts (in number of trajectories)
    auto_save: False

  target_update_freq: 1  # Frequency of target network updates

  # params for rollout
  sampling_params:
    do_sample: True
    temperature_train: 1.0
    temperature_eval: 0.6
    top_k: 50
    top_p: 1.0
    repetition_penalty: 1.0

  # length argument for autoregressive sampling
  length_params:
    max_new_token: 7
    max_length: 1024
    min_length: 1

env:
  group_name: "EnvGroup"

  train:
    total_num_envs: 2
    override_cfg:
      target_ee_pose: [0.5, 0, 0.1, -3.14, 0, -1.57]
  
  eval:
    total_num_envs: 2
    override_cfg:
      is_dummy: True
      target_ee_pose: [0, 0, 0, 0, 0, 0]
      # Dummy placeholders for robot and camera
      robot_ip: 0.0.0.0
      camera_serials: ["0123456789", ]


rollout:
  group_name: "RolloutGroup"
  backend: "huggingface"
  enable_offload: False
  pipeline_stage_num: 1
  sync_weight_nccl_max_ctas: 32 # Max CTAs (Compute Thread Array, the compute resource of GPUs) for sync weight communication. Higher values consume more GPU resources, and thus sync weight is faster but could affect rollout speed in async mode. Positive integer 1 to 32. Default 32.

  collect_transitions: True
  collect_prev_infos: False

  model:
    model_path: "/path/to/model"
    precision: ${actor.model.precision}

  enable_torch_compile: True
actor:
  group_name: "ActorGroup"
  training_backend: "fsdp"

  micro_batch_size: 256  # Smaller batch_size to accelerate training in real world
  global_batch_size: 256  # Smaller batch_size to accelerate training in real world
  seed: 1234
  enable_offload: False

  model:
    model_path: "/path/to/model"
    state_dim: 19
    action_dim: 6
    num_q_heads: 10 # number of Q heads for critic, passed to policy module

  optim:
    clip_grad: 10.0
    lr: 3e-4
    lr_scheduler: torch_constant

  critic_optim:
    clip_grad: 10.0
    lr: 3e-4
    lr_scheduler: torch_constant

  # Override the default values in training_backend/fsdp
  fsdp_config:
    strategy: "fsdp"
    sharding_strategy: "no_shard"
    mixed_precision:
      param_dtype: ${actor.model.precision}
      reduce_dtype: ${actor.model.precision}
      buffer_dtype: ${actor.model.precision}

reward:
  use_reward_model: False

critic:
  use_critic_model: False
